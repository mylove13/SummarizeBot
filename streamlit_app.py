import streamlit as st
import os
import json
import uuid
import csv
from openai import OpenAI

# âœ… API í‚¤ ë¡œë”© (í™˜ê²½ ë³€ìˆ˜ ì‚¬ìš©)
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    st.error("âŒ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
    st.stop()

client = OpenAI(api_key=api_key)

# âœ… ì‚¬ìš©ì ì‹ë³„ (ì„¸ì…˜ ê¸°ë°˜)
if "user_id" not in st.session_state:
    user_id = str(uuid.uuid4())
    st.session_state.user_id = user_id
else:
    user_id = st.session_state.user_id

# âœ… ì‚¬ìš©ì íŒŒì¼ ì €ì¥ ë””ë ‰í† ë¦¬
USER_FILES_DIR = "user_data"
os.makedirs(USER_FILES_DIR, exist_ok=True)

scrap_file = os.path.join(USER_FILES_DIR, f"scrap_{user_id}.json")
summary_file = os.path.join(USER_FILES_DIR, f"summary_{user_id}.json")

if os.path.exists(scrap_file):
    with open(scrap_file, "r", encoding="utf-8") as f:
        scrap_list = json.load(f)
else:
    scrap_list = []

if os.path.exists(summary_file):
    with open(summary_file, "r", encoding="utf-8") as f:
        summary_map = json.load(f)
else:
    summary_map = {}

# âœ… ë‰´ìŠ¤ ë¡œë”©
def load_articles(filename="news_articles.json"):
    if os.path.exists(filename):
        with open(filename, "r", encoding="utf-8") as f:
            return json.load(f)
    else:
        st.error(f"âŒ ë‰´ìŠ¤ íŒŒì¼ {filename}ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return []

articles = load_articles()

# âœ… í•„í„° ì„¤ì •
st.sidebar.title("ğŸ” í•„í„° ì„¤ì •")
all_categories = list(set([a["category"] for a in articles]))
all_sources = list(set([a["source"] for a in articles]))
all_keywords = list(set([kw for a in articles for kw in a.get("keywords", [])]))

selected_categories = st.sidebar.multiselect("ì¹´í…Œê³ ë¦¬ ì„ íƒ", all_categories)
selected_sources = st.sidebar.multiselect("ì–¸ë¡ ì‚¬ ì„ íƒ", all_sources)
selected_keyword = st.sidebar.selectbox("í‚¤ì›Œë“œ ì„ íƒ", ["(ì„ íƒ ì•ˆ í•¨)"] + all_keywords)
search_text = st.sidebar.text_input("ê²€ìƒ‰ì–´ ì…ë ¥")

# âœ… í•„í„° ì ìš©
filtered_articles = [
    a for a in articles if
    (a["category"] in selected_categories if selected_categories else True) and
    (a["source"] in selected_sources if selected_sources else True) and
    (selected_keyword == "(ì„ íƒ ì•ˆ í•¨)" or selected_keyword in a.get("keywords", [])) and
    (search_text.lower() in (a["title"] + a["content"]).lower())
]

# âœ… UI
st.title("ğŸ“¢ AI ë‰´ìŠ¤ ìš”ì•½ & ìŠ¤í¬ë© (ì‚¬ìš©ìë³„ ì €ì¥)")
for article in filtered_articles:
    st.markdown("---")
    st.subheader(f"ğŸ“° {article['title']}")
    st.caption(f"{article['date']} | {article['source']} | ğŸ“‚ {article['category']}")

    if article.get("keywords"):
        st.markdown("**ğŸ”‘ í‚¤ì›Œë“œ:** " + ", ".join(article["keywords"]))

    article_id = article["id"]

    # âœ… ì‚¬ìš©ì ìš”ì•½
    if article_id in summary_map:
        st.success(summary_map[article_id])
    else:
        if st.button(f"ìš”ì•½ ë³´ê¸°", key=f"{article_id}_summary"):
            response = client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": f"ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ 3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì¤˜:\n\n{article['content']}"}]
            )
            summary = response.choices[0].message.content.strip()
            summary_map[article_id] = summary
            with open(summary_file, "w", encoding="utf-8") as f:
                json.dump(summary_map, f, ensure_ascii=False, indent=2)
            st.success(summary)

    # âœ… ì‚¬ìš©ì ìŠ¤í¬ë©
    if article_id in scrap_list:
        st.info("âœ” ì´ë¯¸ ìŠ¤í¬ë©í•œ ë‰´ìŠ¤ì…ë‹ˆë‹¤.")
    else:
        if st.button("ğŸ¤ ìŠ¤í¬ë©", key=f"{article_id}_scrap"):
            scrap_list.append(article_id)
            with open(scrap_file, "w", encoding="utf-8") as f:
                json.dump(scrap_list, f, ensure_ascii=False)
            st.success("ë‰´ìŠ¤ë¥¼ ìŠ¤í¬ë©í–ˆìŠµë‹ˆë‹¤.")

# âœ… ì‚¬ì´ë“œë°”ì— ìŠ¤í¬ë©ëœ ë‰´ìŠ¤ í‘œì‹œ
st.sidebar.title("ğŸ“Œ ìŠ¤í¬ë©ëœ ë‰´ìŠ¤")
for article in articles:
    if article["id"] in scrap_list:
        st.sidebar.write(f"âœ… {article['title']} ({article['date']} | {article['source']})")

# âœ… ì‚¬ìš©ìë³„ ìŠ¤í¬ë© ë‹¤ìš´ë¡œë“œ (CSV)
scrap_info = [{"title": a["title"], "date": a["date"], "source": a["source"]} for a in articles if a["id"] in scrap_list]
scrap_csv = "title,date,source\n" + "\n".join([f"{i['title']},{i['date']},{i['source']}" for i in scrap_info])
st.sidebar.download_button(
    label="ğŸ“¥ ìŠ¤í¬ë©ëœ ë‰´ìŠ¤ ë‹¤ìš´ë¡œë“œ (CSV)",
    data=scrap_csv,
    file_name=f"scrap_info_{user_id}.csv",
    mime="text/csv"
)

# âœ… ì‚¬ìš©ìë³„ ìš”ì•½ ë‹¤ìš´ë¡œë“œ (CSV)
summary_info = [{"title": a["title"], "date": a["date"], "summary": summary_map.get(a["id"])} for a in articles if a["id"] in summary_map]
summary_csv = "title,date,summary\n" + "\n".join([f"{i['title']},{i['date']},{i['summary']}" for i in summary_info])
st.sidebar.download_button(
    label="ğŸ“¥ ìš”ì•½ ë‹¤ìš´ë¡œë“œ (CSV)",
    data=summary_csv,
    file_name=f"summary_info_{user_id}.csv",
    mime="text/csv"
)
