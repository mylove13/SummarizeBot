import streamlit as st
import os
import json
import uuid
from openai import OpenAI
import pandas as pd  # ë°ì´í„°í”„ë ˆì„ ì‚¬ìš©ì„ ìœ„í•´ import

# âœ… API í‚¤ ë¡œë”© (í™˜ê²½ ë³€ìˆ˜ ì‚¬ìš©)
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    st.error(
        "âŒ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ë¥¼ ì„¤ì •í•˜ì„¸ìš”."
    )
    st.stop()

client = OpenAI(api_key=api_key)

# âœ… ì‚¬ìš©ì ì‹ë³„ (HTTP ì¿ í‚¤ ê¸°ë°˜)
user_id = st.session_state.get("user_id", None)


if not user_id:
    user_id = str(uuid.uuid4())
    st.session_state.user_id = user_id
    st.write(
        f'<script>document.cookie = "user_id={user_id}; path=/";</script>',
        unsafe_allow_html=True,
    )

st.sidebar.info(f"í˜„ì¬ ì‚¬ìš©ì ID (ì¿ í‚¤): {user_id}")


# âœ… ì‚¬ìš©ì íŒŒì¼ ì €ì¥ ë””ë ‰í† ë¦¬
USER_FILES_DIR = os.path.join("user_data", user_id)
os.makedirs(USER_FILES_DIR, exist_ok=True)

scrap_file = os.path.join(USER_FILES_DIR, "scrap.json")
summary_file = os.path.join(USER_FILES_DIR, "summary.json")


# âœ… ìŠ¤í¬ë© ë° ìš”ì•½ ë¡œë“œ (íŒŒì¼ì—ì„œ í•­ìƒ ë¡œë“œ)
if "scrap_list" not in st.session_state:
    if os.path.exists(scrap_file):
        try:
            with open(scrap_file, "r", encoding="utf-8") as f:
                st.session_state.scrap_list = json.load(f)
        except json.JSONDecodeError:
            st.error(
                f"âš ï¸ {scrap_file} íŒŒì¼ì´ ì†ìƒë˜ì—ˆìŠµë‹ˆë‹¤. ìŠ¤í¬ë© ëª©ë¡ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."
            )
            st.session_state.scrap_list = []
    else:
        st.session_state.scrap_list = []

if "summary_map" not in st.session_state:
    if os.path.exists(summary_file):
        try:
            with open(summary_file, "r", encoding="utf-8") as f:
                st.session_state.summary_map = json.load(f)
        except json.JSONDecodeError:
            st.error(
                f"âš ï¸ {summary_file} íŒŒì¼ì´ ì†ìƒë˜ì—ˆìŠµë‹ˆë‹¤. ìš”ì•½ ëª©ë¡ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."
            )
            st.session_state.summary_map = {}
    else:
        st.session_state.summary_map = {}

scrap_list = st.session_state.scrap_list
summary_map = st.session_state.summary_map


# âœ… ë‰´ìŠ¤ ë¡œë”©
@st.cache_data
def load_articles(filename="news_articles.json"):
    if os.path.exists(filename):
        try:
            with open(filename, "r", encoding="utf-8") as f:
                return json.load(f)
        except json.JSONDecodeError:
            st.error(f"âš ï¸ {filename} íŒŒì¼ì´ ì†ìƒë˜ì—ˆìŠµë‹ˆë‹¤.  íŒŒì¼ì„ í™•ì¸í•˜ê±°ë‚˜ ë‹¤ì‹œ ìƒì„±í•´ì£¼ì„¸ìš”.")
            return []  # ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•˜ì—¬ ì•±ì´ ì¤‘ë‹¨ë˜ì§€ ì•Šë„ë¡ í•¨
    else:
        st.error(f"âŒ ë‰´ìŠ¤ íŒŒì¼ {filename}ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return []


articles = load_articles()


# âœ… í•„í„° ì„¤ì •
st.sidebar.title("ğŸ” í•„í„° ì„¤ì •")
if articles: # articlesê°€ ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ì—ë§Œ í•„í„° ìƒì„±.
    all_categories = list(set([a["category"] for a in articles]))
    all_sources = list(set([a["source"] for a in articles]))
    all_keywords = list(set([kw for a in articles for kw in a.get("keywords", [])]))

    selected_categories = st.sidebar.multiselect("ì¹´í…Œê³ ë¦¬ ì„ íƒ", all_categories)
    selected_sources = st.sidebar.multiselect("ì–¸ë¡ ì‚¬ ì„ íƒ", all_sources)
    selected_keyword = st.sidebar.selectbox("í‚¤ì›Œë“œ ì„ íƒ", ["(ì„ íƒ ì•ˆ í•¨)"] + all_keywords)
    search_text = st.sidebar.text_input("ê²€ìƒ‰ì–´ ì…ë ¥")

    # âœ… í•„í„° ì ìš©
    filtered_articles = [
        a
        for a in articles
        if (a["category"] in selected_categories if selected_categories else True)
        and (a["source"] in selected_sources if selected_sources else True)
        and (
            selected_keyword == "(ì„ íƒ ì•ˆ í•¨)"
            or selected_keyword in a.get("keywords", [])
        )
        and (search_text.lower() in (a["title"] + a["content"]).lower())
    ]
else:
    filtered_articles = [] # articlesê°€ ë¹„ì–´ìˆìœ¼ë©´, í•„í„°ë§ëœ ê²°ê³¼ë„ ë¹ˆ ë¦¬ìŠ¤íŠ¸.

# âœ… UI
st.title("ğŸ“¢ AI ë‰´ìŠ¤ ìš”ì•½ & ìŠ¤í¬ë© (ì‚¬ìš©ìë³„ ì €ì¥)")
if not filtered_articles:
    st.warning("âš ï¸ í•„í„° ì¡°ê±´ì— ë§ëŠ” ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
else:
    for article in filtered_articles:
        st.markdown("---")
        st.subheader(f"ğŸ“° {article['title']}")
        st.caption(f"{article['date']} | {article['source']} | ğŸ“‚ {article['category']}")

        if article.get("keywords"):
            st.markdown("**ğŸ”‘ í‚¤ì›Œë“œ:** " + ", ".join(article["keywords"]))

        article_id = article["id"]

        # âœ… ì‚¬ìš©ì ìš”ì•½
        if article_id in summary_map:
            st.success(summary_map[article_id])
        else:
            if st.button(f"ìš”ì•½ ë³´ê¸°", key=f"{article_id}_summary"):
                try:
                    response = client.chat.completions.create(
                        model="gpt-4o",
                        messages=[
                            {
                                "role": "user",
                                "content": f"ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ 3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì¤˜:\n\n{article['content']}",
                            }
                        ],
                    )
                    summary = response.choices[0].message.content.strip()
                    summary_map[article_id] = summary
                    with open(summary_file, "w", encoding="utf-8") as f:
                        json.dump(summary_map, f, ensure_ascii=False, indent=2)
                    st.success(summary)
                except Exception as e:
                    st.error(f"âŒ ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

        # âœ… ì‚¬ìš©ì ìŠ¤í¬ë©
        if article_id in scrap_list:
            st.info("âœ” ì´ë¯¸ ìŠ¤í¬ë©í•œ ë‰´ìŠ¤ì…ë‹ˆë‹¤.")
        else:
            if st.button("ğŸ¤ ìŠ¤í¬ë©", key=f"{article_id}_scrap"):
                scrap_list.append(article_id)
                with open(scrap_file, "w", encoding="utf-8") as f:
                    json.dump(scrap_list, f, ensure_ascii=False)
                st.success("ë‰´ìŠ¤ë¥¼ ìŠ¤í¬ë©í–ˆìŠµë‹ˆë‹¤.")

# âœ… ì‚¬ì´ë“œë°”ì— ìŠ¤í¬ë©ëœ ë‰´ìŠ¤ í‘œì‹œ
st.sidebar.title("ğŸ“Œ ìŠ¤í¬ë©ëœ ë‰´ìŠ¤")
if scrap_list: # ìŠ¤í¬ë©ëœ ë‰´ìŠ¤ê°€ ìˆì„ ê²½ìš°ì—ë§Œ í‘œì‹œ
    for article in articles:
        if article["id"] in scrap_list:
            st.sidebar.write(
                f"âœ… {article['title']} ({article['date']} | {article['source']})"
            )
else:
    st.sidebar.write("ìŠ¤í¬ë©ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")

# âœ… ì‚¬ìš©ìë³„ ìŠ¤í¬ë© ë‹¤ìš´ë¡œë“œ (CSV)
st.sidebar.title("â¬‡ï¸ ë‹¤ìš´ë¡œë“œ") # ë‹¤ìš´ë¡œë“œ ì„¹ì…˜ ì œëª© ì¶”ê°€
if scrap_list:
    scrap_info = [
        {"title": a["title"], "date": a["date"], "source": a["source"]}
        for a in articles
        if a["id"] in scrap_list
    ]
    scrap_df = pd.DataFrame(scrap_info)  # pandas DataFrameìœ¼ë¡œ ë³€í™˜
    scrap_csv = scrap_df.to_csv(index=False)  # index ì œì™¸í•˜ê³  CSV ìƒì„±
    st.sidebar.download_button(
        label="ğŸ“¥ ìŠ¤í¬ë©ëœ ë‰´ìŠ¤ ë‹¤ìš´ë¡œë“œ (CSV)",
        data=scrap_csv,
        file_name=f"scrap_info_{user_id}.csv",
        mime="text/csv",
    )

# âœ… ì‚¬ìš©ìë³„ ìš”ì•½ ë‹¤ìš´ë¡œë“œ (CSV)
if summary_map:
    summary_info = [
        {"title": a["title"], "date": a["date"], "summary": summary_map.get(a["id"], "ìš”ì•½ ì—†ìŒ")}
        for a in articles
        if a["id"] in summary_map # summary_mapì— ìˆëŠ” ê²ƒë§Œ ì²˜ë¦¬.
    ]

    summary_df = pd.DataFrame(summary_info)  # pandas DataFrameìœ¼ë¡œ ë³€í™˜
    summary_csv = summary_df.to_csv(index=False)
    st.sidebar.download_button(
        label="ğŸ“¥ ìš”ì•½ ë‹¤ìš´ë¡œë“œ (CSV)",
        data=summary_csv,
        file_name=f"summary_info_{user_id}.csv",
        mime="text/csv",
    )
